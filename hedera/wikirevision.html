<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">    

    <title>Hedera: Wikipedia Revision</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="css/bootstrap-theme.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="theme.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body role="document">

    <!-- Fixed navbar -->
    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Home</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="about.html">About</a></li>
            <li class="active"><a href="wikirevision.html">Wikipedia Revision</a></li>
            <li><a href="https://github.com/antoine-tran/Hedera">Source code</a></li>
            <li><a href="apidocs/index.html">API</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container theme-showcase" role="main">

      <!-- Main jumbotron for a primary marketing message or call to action -->
      <header class="jumbotron subhead">
        <div class="container">
          <h1>Hedera</h1>
          <p>A Hadoop toolkit for processing large versioned document collections</p>
        </div>
      </header>

      <div class="page-header">
        <h1>Getting started with Wikipedia Revision</h1>
      </div>
      
    <p>
Wikipedia is perhaps one of the most popular datasets among the research community of text mining. Wikipedia has several unique features distinguishing it from other dataset: It contains rich information compiled by human, it covers a wide range of topics, from scientific knowledge to celebrity's bibliography and so on, and it is free. Compared to Wikiepdia text, Wikipedia revision history includes a much broader spectrum of data: Articles, talks, discussions, etc. It also contains a complete set of snapshots for each Wikipedia pages, making it aa gold mine for many studies that are concerned with the dynamics of Wikipedia contents over time.
</p>
<p>Unfortunately, while there are several tools supporting processing Wikipedia text dumps in centralized and distributed settings (for example, <a href="http://lintool.github.io/Cloud9/docs/content/wikipedia.html">Cloud<sup>9</sup></a> is an excellent tool that is based on Hadoop framework), the access to the Wikipedia revision history is still prohibited because of their enormous volumes (tens of Terabytes of text). Wikipedia revision history datasets are formatted in a different way from Wikipedia text, and it contains several compressed files, which makes tools such as Cloud<sup>9</sup> difficult to re-use.
</p>
<p>
Some other software tools such as <a href="https://code.google.com/p/jwpl">JWPL Revision machine</a> makes a considerable effort to repack the Wikipedia revision history dataset by storing only differences between consecutive revisions, which saves a lot of hard disk capacity (for instance, the dump versioned on 2014 May 02 amounts only 380 GBytes of hard disk). While in many cases this is sufficient to get basic structures from the dataset, such as linking network, it is unable to make text-intensive queries (for instance, full-text over a sequence of revisions of different articles) without re-constructing the text from the beginning revisions and thus result in a huge bottleneck, if not killing the database server. What is more, JWPL lacks the flexibility support to extend its functionalities, even though the authors make quite an effort to provide as rich family of APIs as possible. It is thus impossible to extract ad-hoc information from Wikipedia revision (for example, extracting all anchor texts of all revisions belonging to a certain categories). Finally, JWPL is a centralized setting, it takes very long time to preprocess (one month to index the dataset) before the first experiment / testing code can be called. In several research scenarios, this is unacceptable.
</p>

<p>
And that is where Hedera comes in.
</p>

<p>
With Hedera, we try to make Wikipedia revision history dataset accessible to users of different domains and demands. Instead of providing a full-fledged (and thus cumbersome, difficult to extend) indexing and storing architecture, we built Hedera as an incremental processing tool for Wikipedia revisions. Users can quickly process on subset of Wikipedia revisions (for example, a few dump files instead of all, or just work with articles)
</p>


<code>
jobConfigurationObject.setBoolean(WikiRevisionInputFormat.SKIP_NON_ARTICLES, true);
</code>

or in Pig:

<code>
SET 'org.hedera.input.onlyarticle' true
</code>


<p>
The revision history dump contains all text in Wikimedia projects from the beginning (2001) till the date of the dumps. You can specify one particular date of your choice with the following code snippets:
</p>

<h3>Example in Java</h3>
<pre>
<code>
hadoop jar hedera-0.X-SNAPSHOT.jar [CLASS_NAME] -begin [TIME1] -end [TIME2]
</code>
</pre>

<h3>Example in Pig</h3>
<pre>
<code>
SET 'org.hedera.input.begintime' TIME1
SET 'org.hedera.input.endtime' TIME2
</code>
</pre>

where TIME1 and TIME2's are strings in ISOTime format.

      <div class="page-header">
         <h1>Building Dictionary, Vectorization</h1>
      </div>

<p><i>(this is still tentative. To be updated soon)</i></p>

<p>In Hedera, you can also build basic data structures for Information Retrieval / matrix-like computation, such as mappings of entity / revision / texts into continous ID arrays. </p>


<h3> Flattening dumps to CSV files </h3>

One of the favourite format for handling text in different programming languages (e.g. Python) is plain-text CSV files. In Hedera you can flatten the XML dumps into CSV files simply by calling the Pig script XML2JSON.pig in source directory <code>pig/utils</code>. The output is the set of .csv files, each containing one revision per line in JSON format with the following schema:


<pre>
<code>
{
 "page_id":NUMBER,
 "page_title":STRING,
 "page_namespace":NUMBER,
 "rev_id":NUMBER,
 "parent_id":NUMBER,
 "timestamp":NUMBER,
 "user":STRING,
 "user_id":NUMBER,
 "comment":STRING,
 "text":STRING
}
</code>
</pre>


<<<<<<< HEAD

    </div> <!-- /container -->
=======
    </div>
>>>>>>> 2621939f8f124969ebaaf8c9c8269d580a3c4c83


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <script src="../../assets/js/docs.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
